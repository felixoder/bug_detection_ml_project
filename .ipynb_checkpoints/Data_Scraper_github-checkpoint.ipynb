{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ziUe7fiQJ0a"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SU-176MwQfEo"
   },
   "outputs": [],
   "source": [
    "# GITHUB_TOKEN = add_your_token\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
    "SEARCH_URL = \"https://api.github.com/search/commits\"\n",
    "QUERY_LIST = [\"fix OR bug OR error\", \"patch\", \"syntax error\", \"issue fixed\", \"refactor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5WjCukbQhgn"
   },
   "outputs": [],
   "source": [
    "def search_commits(keyword, language=\"Python\", per_page=100, page=1):\n",
    "    url = f\"{SEARCH_URL}?q={keyword}+language:{language}&sort=author-date&order=desc&per_page={per_page}&page={page}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "\n",
    "    if response.status_code == 403:  # Rate limit exceeded\n",
    "        print(\"Rate limit hit! Sleeping for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        return []\n",
    "\n",
    "    return response.json().get(\"items\", []) if response.status_code == 200 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUZ_uKDGQiWo"
   },
   "outputs": [],
   "source": [
    "def get_code_diff(repo_full_name, commit_sha):\n",
    "    url = f\"https://api.github.com/repos/{repo_full_name}/commits/{commit_sha}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    if response.status_code != 200:\n",
    "        return None, None\n",
    "\n",
    "    files = response.json().get(\"files\", [])\n",
    "    buggy_code, fixed_code = [], []\n",
    "\n",
    "    for file in files:\n",
    "        if file[\"filename\"].endswith(\".py\"):  # Filter only Python files\n",
    "            patch = file.get(\"patch\", \"\")\n",
    "\n",
    "            # Clean up Git diff metadata\n",
    "            cleaned_patch = re.sub(r'@@.*?@@', '', patch)  # Remove diff headers\n",
    "            cleaned_patch = re.sub(r'^\\+', '', cleaned_patch, flags=re.MULTILINE)  # Remove '+'\n",
    "            cleaned_patch = re.sub(r'^-', '', cleaned_patch, flags=re.MULTILINE)  # Remove '-'\n",
    "\n",
    "            # Extract buggy and fixed code separately\n",
    "            buggy_lines = [line[1:] for line in patch.split(\"\\n\") if line.startswith(\"-\")]\n",
    "            fixed_lines = [line[1:] for line in patch.split(\"\\n\") if line.startswith(\"+\")]\n",
    "\n",
    "            if buggy_lines and fixed_lines:\n",
    "                buggy_code.append(\"\\n\".join(buggy_lines))\n",
    "                fixed_code.append(\"\\n\".join(fixed_lines))\n",
    "\n",
    "    return \"\\n\".join(buggy_code), \"\\n\".join(fixed_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1fEd3KAQmj1"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "MAX_COMMITS = 100000\n",
    "fetched_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNEEYdK6QpEI",
    "outputId": "d5939fed-dc48-4eea-f4ca-39509f36dc4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching commits with query: fix OR bug OR error (Page 1)\n",
      "Fetching commits with query: fix OR bug OR error (Page 2)\n",
      "Rate limit hit! Sleeping for 60 seconds...\n",
      "Fetching commits with query: patch (Page 1)\n",
      "Fetching commits with query: patch (Page 2)\n",
      "Fetching commits with query: patch (Page 3)\n",
      "Fetching commits with query: patch (Page 4)\n",
      "Fetching commits with query: patch (Page 5)\n",
      "Fetching commits with query: patch (Page 6)\n",
      "Fetching commits with query: patch (Page 7)\n",
      "Fetching commits with query: patch (Page 8)\n",
      "Fetching commits with query: patch (Page 9)\n",
      "Fetching commits with query: patch (Page 10)\n",
      "Fetching commits with query: patch (Page 11)\n",
      "Fetching commits with query: syntax error (Page 1)\n",
      "Fetching commits with query: syntax error (Page 2)\n",
      "Fetching commits with query: syntax error (Page 3)\n",
      "Fetching commits with query: issue fixed (Page 1)\n",
      "Fetching commits with query: issue fixed (Page 2)\n",
      "Fetching commits with query: refactor (Page 1)\n",
      "Fetching commits with query: refactor (Page 2)\n",
      "Fetching commits with query: refactor (Page 3)\n",
      "Fetching commits with query: refactor (Page 4)\n",
      "Fetching commits with query: refactor (Page 5)\n",
      "Fetching commits with query: refactor (Page 6)\n",
      "Fetching commits with query: refactor (Page 7)\n",
      "Fetching commits with query: refactor (Page 8)\n",
      "Fetching commits with query: refactor (Page 9)\n"
     ]
    }
   ],
   "source": [
    "for query in QUERY_LIST:\n",
    "    page = 1\n",
    "    while fetched_count < MAX_COMMITS:\n",
    "        print(f\"Fetching commits with query: {query} (Page {page})\")\n",
    "        commits = search_commits(query, per_page=100, page=page)\n",
    "\n",
    "        if not commits:\n",
    "            break  # No more commits to fetch\n",
    "\n",
    "        for commit in commits:\n",
    "            if fetched_count >= MAX_COMMITS:\n",
    "                break\n",
    "\n",
    "            repo_name = commit[\"repository\"][\"full_name\"]\n",
    "            commit_sha = commit[\"sha\"]\n",
    "            buggy, fixed = get_code_diff(repo_name, commit_sha)\n",
    "\n",
    "            if buggy and fixed:\n",
    "                data.append((buggy, \"buggy\"))\n",
    "                data.append((fixed, \"bug-free\"))\n",
    "                fetched_count += 2\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(2)  # Avoid hitting rate limits for the github api [TODO optimize it ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0iHvcUuRW8c",
    "outputId": "5e084ed8-34c0-46e0-c08a-b74b7fd8e5f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as github_scraped.csv with 486 samples.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset\n",
    "df = pd.DataFrame(data, columns=[\"code\", \"label\"])\n",
    "df.to_csv(\"github_scraped.csv\", index=False)\n",
    "\n",
    "print(f\"Dataset saved as github_scraped.csv with {len(df)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "B6hmIookR2YZ",
    "outputId": "9d94fe71-8b97-4b4c-8f2a-c17337eeb818"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_43f48cef-566e-4ac5-8e92-2a546dc1b015\", \"github_scraped.csv\", 2843408)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('github_scraped.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
